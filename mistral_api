import requests

url = "http://192.168.19.1:1234/v1/chat/completions"

def generate_mistral_response(prompt, max_tokens=2000):
    print("[ðŸš€] Generating response from Mistral-7B-Instruct via LM Studio...")
    headers = {
        "Content-Type": "application/json",
    }
    data = {
        "model": "mistral-7b-instruct-v0.3",
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "max_tokens": max_tokens,
        "temperature": 0.9
    }

    response = requests.post(url, headers=headers, json=data)
    answer = response.json()["choices"][0]["message"]["content"]
    print(f"[âœ…] Response: {answer[:200]}...")
    return answer
